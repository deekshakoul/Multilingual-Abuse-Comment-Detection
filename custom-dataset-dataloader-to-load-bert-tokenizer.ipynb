{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* In continuation with post: [tokenize-train-data-using-bert-tokenizer](https://www.kaggle.com/harveenchadha/tokenize-train-data-using-bert-tokenizer/notebook)\n\n* After you save the tokenizers output in npy format, following code could be implemented in order to reloading as torch.data.Dataset and then passing it to a dataloader.\n\n* UseCase:-\nLarge training data so to avoid on the fly tokenizatio, we save tokenized data to disk and load it using below code.\n","metadata":{}},{"cell_type":"code","source":"train_token = np.load('token_train.npy', allow_pickle=True)\n#the BERT tokenization is saved as token_train.npy, see the post to find how it is saved\ntrain_token=train_token.tolist()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(token_train.keys())\n# O/P - dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'label'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    \n    def __init__(self, dict_,  labels =False):\n        self.dict = dict_\n        self.labels = labels\n    def __getitem__(self, ids):\n\n        return {\n            'input_ids' : torch.tensor(self.dict['input_ids'][ids], dtype=torch.long),\n            'token_type_ids' : torch.tensor(self.dict['token_type_ids'][ids], dtype=torch.long),\n            'attention_mask' : torch.tensor(self.dict['attention_mask'][ids], dtype=torch.long),\n            'labels' : torch.tensor(self.dict['label'][ids], dtype=torch.long)\n        }\n    \n    def __len__(self):\n        return len(self.dict['label'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After fetching a list of samples using the indices[ids here] from sampler, the function passed as the collate_fn argument is used to collate lists of samples into batches","metadata":{}},{"cell_type":"code","source":"def collate(batch):\n    ips = [item['input_ids'] for item in batch]\n    ttypes = [item['token_type_ids'] for item in batch]\n    attn = [item['attention_mask'] for item in batch]\n    lb = [item['labels'] for item in batch]\n\n    return {\n               'token_type_ids': torch.stack(ttypes),\n               'input_ids': torch.stack(ips),\n               'attention_mask' : torch.stack(attn),\n               'labels': torch.stack(lb) \n            }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = MyDataset(train_token,)\ntrain_dataloader =  DataLoader(dataset,batch_size=128, collate_fn= collate, shuffle=True)","metadata":{},"execution_count":null,"outputs":[]}]}