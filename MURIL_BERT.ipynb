{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deekshakoul/Multilingual-Abuse-Comment-Detection/blob/main/MURIL_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad045665-89b5-4a11-a022-3eb2e28f0dc6",
      "metadata": {
        "id": "ad045665-89b5-4a11-a022-3eb2e28f0dc6"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'seed': 42,\n",
        "    'test_size': 0.1\n",
        "    'model':'google/muril-base-cased',\n",
        "    'batch_size' : 32,\n",
        "    'num_epochs' : 1,\n",
        "    'max_length' : 64,\n",
        "    'adam_lr' : 5e-5   \n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9b4a2a8-1aa9-4996-9f35-981591ac2921",
      "metadata": {
        "id": "f9b4a2a8-1aa9-4996-9f35-981591ac2921"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import re\n",
        "import unidecode\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import AdamW\n",
        "from transformers import get_scheduler\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ece2e2eb-256d-4743-9bc3-47e0b34126ba",
      "metadata": {
        "id": "ece2e2eb-256d-4743-9bc3-47e0b34126ba"
      },
      "source": [
        "**About the data**\n",
        "\n",
        "As the data is huge, we implement some processing to get sufficient and relevant data only.\n",
        "* removed emojis from text. Noticed that most of time the comment with only emojis were largely non-abusive(label 0) and hence removed emojis and then checked the length of text avilable.\n",
        "* remove any links, digits and punctuation.\n",
        "* only used those  data that len between 3 and 16 (inclusive), onfirmed via the distribution of text lengths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "697870e3-5253-467e-af64-6ff36a0befad",
      "metadata": {
        "id": "697870e3-5253-467e-af64-6ff36a0befad"
      },
      "outputs": [],
      "source": [
        "train = 'ShareChat-IndoML-Datathon-NSFW-CommentChallenge_Train.csv'\n",
        "test = 'ShareChat-IndoML-Datathon-NSFW-CommentChallenge_Test_20_Percent_NoLabel.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading train and test data"
      ],
      "metadata": {
        "id": "YUMM0Y6Pdxh_"
      },
      "id": "YUMM0Y6Pdxh_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4890e071-6b17-45b1-ae97-37b20e31e417",
      "metadata": {
        "id": "4890e071-6b17-45b1-ae97-37b20e31e417"
      },
      "outputs": [],
      "source": [
        "with open (train, 'r') as f:\n",
        "    t = f.readlines()\n",
        "with open (test, 'r') as f:\n",
        "    tt = f.readlines()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bb354df-18fa-4f9e-a52b-b3f8f52213f4",
      "metadata": {
        "id": "3bb354df-18fa-4f9e-a52b-b3f8f52213f4"
      },
      "outputs": [],
      "source": [
        "txts = []\n",
        "ids = []\n",
        "labels = []\n",
        "skip = 0\n",
        "for i  in range(1, len(t)):\n",
        "    if t[i][-2] in [\"0\", \"1\"]:\n",
        "#         x = t[i].split(\",\")[0]\n",
        "        txts.append(t[i][:-3])\n",
        "        ids.append(i-1)\n",
        "        labels.append(int(t[i][-2]))\n",
        "    else:\n",
        "        skip += 1\n",
        "df = pd.DataFrame({ 'ids': ids, 'text': txts, 'label': labels})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db06d635-b934-4e69-9bd3-53dca337575b",
      "metadata": {
        "id": "db06d635-b934-4e69-9bd3-53dca337575b"
      },
      "outputs": [],
      "source": [
        "def cleaning(text):\n",
        "#     text = unidecode.unidecode(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(',(?!(?=[^\"]*\"[^\"]*(?:\"[^\"]*\"[^\"]*)*$))', \" \", text)\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = re.sub(r'[0-9]+', '', text) #digits removal\n",
        "#     text =  re.sub(r'http\\S+', ' ', text)\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a2939e6-36e9-43aa-b087-92d76e295166",
      "metadata": {
        "id": "1a2939e6-36e9-43aa-b087-92d76e295166"
      },
      "outputs": [],
      "source": [
        "test = []\n",
        "tids = []\n",
        "for i in range(1,len(tt)):\n",
        "    tids.append(int(tt[i].split(\",\")[0]))\n",
        "    test.append(cleaning(tt[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2b124da-68d0-42a9-8582-7a32152e5ab9",
      "metadata": {
        "id": "e2b124da-68d0-42a9-8582-7a32152e5ab9"
      },
      "outputs": [],
      "source": [
        "df['final'] = df['text'].apply(lambda row: cleaning(row))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Emoji removal using package UNICODE_EMOJI"
      ],
      "metadata": {
        "id": "Cy8x15s-eFwa"
      },
      "id": "Cy8x15s-eFwa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62d07cab-9fcb-4c36-af70-bf489ccf130c",
      "metadata": {
        "id": "62d07cab-9fcb-4c36-af70-bf489ccf130c"
      },
      "outputs": [],
      "source": [
        "from emoji import UNICODE_EMOJI\n",
        "def remove_emojis(s): \n",
        "    return ''.join(c for c in s if c not in UNICODE_EMOJI['en'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3c3312b-e123-4d44-ae72-c99ec9270147",
      "metadata": {
        "id": "f3c3312b-e123-4d44-ae72-c99ec9270147"
      },
      "outputs": [],
      "source": [
        "df['f'] = df['final'].apply(remove_emojis)\n",
        "df['len'] = df['f'].apply(lambda s: len(s.strip().split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selection of data based on their text lengths"
      ],
      "metadata": {
        "id": "RFFsJvfBeSPp"
      },
      "id": "RFFsJvfBeSPp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf689a67-eac8-4cce-9f89-3c5f325b3e70",
      "metadata": {
        "id": "bf689a67-eac8-4cce-9f89-3c5f325b3e70"
      },
      "outputs": [],
      "source": [
        "df = df[(df['len'] > 2) & (df['len'] < 17)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b949d528-5d7b-4d4a-bd70-5f0c8a9eb46b",
      "metadata": {
        "id": "b949d528-5d7b-4d4a-bd70-5f0c8a9eb46b"
      },
      "outputs": [],
      "source": [
        "df_train, df_valid = train_test_split(df, \n",
        "                                      shuffle=True, \n",
        "                                      random_state= config['seed'], \n",
        "                                      test_size=  config['test_size'], \n",
        "#                                       stratify=df['language'].values)\n",
        "                                      stratify=df['label'].values)\n",
        "\n",
        "df_train = df_train.reset_index(drop=True)\n",
        "df_valid = df_valid.reset_index(drop=True)\n",
        "df_train.to_csv('df_train.csv', index=False)\n",
        "df_valid.to_csv('df_valid.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33d8e264-946c-4c6c-9b4d-51fb2e4b3f96",
      "metadata": {
        "id": "33d8e264-946c-4c6c-9b4d-51fb2e4b3f96"
      },
      "source": [
        "# Preprocessing \n",
        "\n",
        "Main tool is tokenizer\n",
        "    \n",
        "    - split text in words: tokens \n",
        "    - tokens into numbers - tensor\n",
        "    - add additional inputs that our model needs(special tokens)\n",
        " \n",
        "Ex. \n",
        "```\n",
        "encoded_input = tokenizer(\"Hello, I'm a single sentence!\")\n",
        "print(encoded_input)\n",
        "\n",
        "{'input_ids': [101, 138, 18696, 155, 1942, 3190, 1144, 1572, 13745, 1104, 159, 9664, 2107, 102], \n",
        "     'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
        "     'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
        "```  \n",
        "* input_ids -  indices corresponding to each token\n",
        "* token_type_ids\n",
        "* attention_mask\n",
        "\n",
        "Decoder example - \n",
        "```\n",
        "tokenizer.decode(encoded_input[\"input_ids\"]) \n",
        "Outputs - \"[CLS] Hello, I'm a single sentence! [SEP]\"\n",
        "```\n",
        "\n",
        "The tokenizer automatically added some special tokens that the model expects. If you don't want any additional tokens set add_special_tokens=False and add special tokens on your own.\n",
        "\n",
        "If you have several sentences you want to process, you can do this efficiently by sending them as a list to the tokenizer.\n",
        "\n",
        "**Note - If you plan on using a pretrained model, itâ€™s important to use the associated pretrained tokenizerb**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f85eb9b-4a46-482d-8377-a408a77256d5",
      "metadata": {
        "id": "6f85eb9b-4a46-482d-8377-a408a77256d5"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(config['model'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET and DATALOADER"
      ],
      "metadata": {
        "id": "-aB7sHGofeRw"
      },
      "id": "-aB7sHGofeRw"
    },
    {
      "cell_type": "markdown",
      "id": "174259c1-765b-4d85-9635-22f79a683e2e",
      "metadata": {
        "id": "174259c1-765b-4d85-9635-22f79a683e2e"
      },
      "source": [
        "Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples\n",
        "\n",
        "Follow this tutorial to create custom dataset on [pytorch](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files)\n",
        "\n",
        "I have created a custom dataset class with the necessary three functions that are : \\_\\_init__, \\_\\_len__, and \\_\\_getitem__. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56c3a371-82ad-4a0d-b310-f38a02c2fbbd",
      "metadata": {
        "id": "56c3a371-82ad-4a0d-b310-f38a02c2fbbd"
      },
      "outputs": [],
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, text, target, tokenizer):\n",
        "        self.max_length = config['max_length']\n",
        "        self.dict = tokenizer(text, max_length = self.max_length, padding='max_length', truncation=True)\n",
        "        self.target = target\n",
        "    def __len__(self):\n",
        "        return len(self.dict['input_ids'])\n",
        "    \n",
        "    def __getitem__(self, ids):\n",
        "        if (self.target is None):\n",
        "            return {\n",
        "            'input_ids' : torch.tensor(self.dict['input_ids'][ids], dtype=torch.long),\n",
        "            'token_type_ids' : torch.tensor(self.dict['token_type_ids'][ids], dtype=torch.long),\n",
        "            'attention_mask' : torch.tensor(self.dict['attention_mask'][ids], dtype=torch.long),\n",
        "        }\n",
        "        else :\n",
        "            return {\n",
        "            'input_ids' : torch.tensor(self.dict['input_ids'][ids], dtype=torch.long),\n",
        "            'token_type_ids' : torch.tensor(self.dict['token_type_ids'][ids], dtype=torch.long),\n",
        "            'attention_mask' : torch.tensor(self.dict['attention_mask'][ids], dtype=torch.long),\n",
        "            'labels' : torch.tensor(self.target[ids], dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a97068d-6718-4ce2-8fdc-d5f2e202d84a",
      "metadata": {
        "id": "4a97068d-6718-4ce2-8fdc-d5f2e202d84a"
      },
      "outputs": [],
      "source": [
        "train_text, train_target = df_train['final'].to_list(), df_train['label'].to_list()\n",
        "eval_text, eval_target = df_valid['final'].to_list(), df_valid['label'].to_list()\n",
        "# test_text = df_test['final'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ebccdef-5601-45c4-8976-de539fdab889",
      "metadata": {
        "id": "4ebccdef-5601-45c4-8976-de539fdab889"
      },
      "outputs": [],
      "source": [
        "train_dataset = BERTDataset(train_text, train_target, tokenizer)\n",
        "eval_dataset = BERTDataset(eval_text, eval_target, tokenizer)\n",
        "test_dataset = BERTDataset(test_text, None, tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=config['batch_size'])\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the dataset is huge the tokenization can take a lot of ram, hence further training of model wouldn't be possible in kaggle kernel. I have the saved the tokenization result too. [Code is taken from [Kaggle Post](https://www.kaggle.com/harveenchadha/tokenize-train-data-using-bert-tokenizer/notebook)]. If you also want to save tokenization and then reload dataset, I have included that code as well in a different notebook[find here to reload the saved tokenization(POST)[https://www.kaggle.com/deekoul/custom-dataset-dataloader-to-load-bert-tokenizer/]."
      ],
      "metadata": {
        "id": "vrkIRjezg9Ad"
      },
      "id": "vrkIRjezg9Ad"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a11216a3-17b4-4966-81ac-9f87ada07abc",
      "metadata": {
        "id": "a11216a3-17b4-4966-81ac-9f87ada07abc"
      },
      "outputs": [],
      "source": [
        "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=config['max_length']):\n",
        "    input_ids = []\n",
        "    tt_ids = []\n",
        "    at_ids = []\n",
        "    \n",
        "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
        "        text_chunk = texts[i:i+chunk_size]\n",
        "        encs = tokenizer(\n",
        "                    text_chunk,\n",
        "                    max_length = maxlen,\n",
        "                    padding='max_length',\n",
        "                    truncation=True\n",
        "                    )\n",
        "        \n",
        "        input_ids.extend(encs['input_ids'])\n",
        "        tt_ids.extend(encs['token_type_ids'])\n",
        "        at_ids.extend(encs['attention_mask'])\n",
        "    \n",
        "    return {'input_ids': input_ids, 'token_type_ids': tt_ids, 'attention_mask':at_ids}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "708331cc-2bfe-401e-b802-fafaa8db0626",
      "metadata": {
        "id": "708331cc-2bfe-401e-b802-fafaa8db0626"
      },
      "outputs": [],
      "source": [
        "token_train = fast_encode(list(df_train['f'].values), tokenizer)\n",
        "token_train['label'] = list(df_train['label'].values)\n",
        "token_valid = fast_encode(list(df_valid['f'].values), tokenizer)\n",
        "token_valid['label'] = list(df_valid['label'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4bb0852-ed69-4d4c-ae76-e3c2e8c92526",
      "metadata": {
        "id": "c4bb0852-ed69-4d4c-ae76-e3c2e8c92526"
      },
      "outputs": [],
      "source": [
        "token_train.keys(), token_valid.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SAVING TOKENIZATION.\n",
        "\n",
        "Please check the post on how to reload it in train_dataloader and valid_dataloader."
      ],
      "metadata": {
        "id": "bMZnMCvvhr1B"
      },
      "id": "bMZnMCvvhr1B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36221114-68f5-4074-88b8-8438f458a666",
      "metadata": {
        "id": "36221114-68f5-4074-88b8-8438f458a666"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.save('token_train.npy', token_train )\n",
        "np.save('token_valid.npy', token_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiated the model"
      ],
      "metadata": {
        "id": "QkdrpIW-h8if"
      },
      "id": "QkdrpIW-h8if"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73e4f19f-b59f-4118-851c-644e66143409",
      "metadata": {
        "id": "73e4f19f-b59f-4118-851c-644e66143409"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(config['model'], num_labels=2)\n",
        "model.to(device) #only if using GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06b36a2f-08be-4e21-adf5-8a79b0194d1c",
      "metadata": {
        "id": "06b36a2f-08be-4e21-adf5-8a79b0194d1c"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "for i in train_dataloader:\n",
        "    print(i['labels'].shape)\n",
        "    break\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a973f576-ab22-4dec-a693-dde11a0fb627",
      "metadata": {
        "id": "a973f576-ab22-4dec-a693-dde11a0fb627"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=config['adam_lr'])\n",
        "num_training_steps = config['num_epochs'] * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ab4c837-ddb6-4d91-a553-6b0acd13c627",
      "metadata": {
        "id": "3ab4c837-ddb6-4d91-a553-6b0acd13c627"
      },
      "outputs": [],
      "source": [
        "progress_bar = tqdm(range(num_training_steps))\n",
        "model.train()\n",
        "for epoch in range(config['num_epochs']):\n",
        "    print(\"epoch - \", epoch)\n",
        "    tr_loss = 0.0\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k : v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "        tr_loss += loss.item()\n",
        "    print('training done with loss- ', tr_loss)\n",
        "    # check validation\n",
        "    a_,b_ = evaluate(model, eval_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c8df7d0-a056-4a5a-b91a-a238be201b53",
      "metadata": {
        "tags": [],
        "id": "5c8df7d0-a056-4a5a-b91a-a238be201b53"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, eval_dataloader):\n",
        "    model.eval()\n",
        "    vloss_final = 0.0\n",
        "    for i,batch in enumerate(eval_dataloader):\n",
        "        batch = {k : v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "        logits = outputs.logits \n",
        "        predictions = torch.argmax(logits, dim=-1)#batch, label\n",
        "        vloss = outputs.loss\n",
        "        if  i == 0:\n",
        "            predict_ = predictions\n",
        "            truth =  batch['labels']\n",
        "            continue\n",
        "        predict_ = torch.cat( (predict_, predictions) )\n",
        "        truth = torch.cat( (truth, batch['labels']) )\n",
        "        vloss_final += vloss.item()\n",
        "    print(\"validation done\")\n",
        "    f1 = f1_score(predict_, truth)\n",
        "    print(f1, vloss_final)\n",
        "    return predict_, truth"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "stanfordnlp",
      "language": "python",
      "name": "stanfordnlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "name": "MURIL-BERT.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
